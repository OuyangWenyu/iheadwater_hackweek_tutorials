{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afa7c42d-6ae4-4af5-b437-22a14e9d1b70",
   "metadata": {},
   "source": [
    "# torch.nn简介\n",
    "\n",
    "pytorch提供了很多设计良好的模块来帮助构建和训练神经网络。为了充分利用它们的能力并为自己的问题定制神经网络，需要了解它们都在做什么，其中被用于构建神经网络 的torch.nn 模块就是我们要重点了解的。之前说到autograd可以实现反向传播功能, 但是如果直接用它来写神经网络的代码还是稍显复杂，而torch.nn 是专门为神经网络设计的模块化接口，它构建于 autograd 之上, 可用来定义和运行神经网络，使用它构建神经网络就会很简单。关于torch.nn的详细介绍，推荐大家查看官方文档介绍：[WHAT IS TORCH.NN REALLY?](https://pytorch.org/tutorials/beginner/nn_tutorial.html)，这里我们从简介绍。\n",
    "\n",
    "## 1 以一个线性层为例简单说明\n",
    "\n",
    "我们知道神经网络典型的一层是一个线性组合加一个非线性激活函数。以线性组合为例，如果我们需要自己手写，那么就会像下面这样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "84b8bb95-2390-4160-b7f8-bb5a5ca76775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f3f31e39-3d79-44c9-b16d-50c41318d694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# nn_sample是一个简单的示例数据文件\n",
    "x_train = torch.Tensor(np.load(\"../data/nn_sample.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "71fd4b20-ece7-4dd9-96ae-76935a7b8803",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.randn(784, 10) / math.sqrt(784)\n",
    "weights.requires_grad_()\n",
    "bias = torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e2288ff7-ad8c-43ce-baf6-a46d90a406c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x):\n",
    "    return x @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fbfd4085-c4b5-41d0-9083-3004d0945142",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = linear(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a22f1e83-1af4-4d06-9421-3b0a0d75c3d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2192, -0.0217, -0.7179, -0.1555, -0.3519, -0.1276, -0.2685, -0.0413,\n",
       "        -0.5878, -0.2578], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9e2c06-f175-4d97-aafa-6c4d4e2e455f",
   "metadata": {},
   "source": [
    "假如样本真实输出都是1："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aeeeef59-623c-48af-b298-be146829f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.ones(preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32637bae-4382-4b0a-8420-b7d102380edf",
   "metadata": {},
   "source": [
    "我们定义loss函数是："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f2625d35-060f-4171-aa05-d7a4f791be7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(p, o):\n",
    "    return torch.mean((p - o) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80535de-17e0-4ae4-b0f6-89988107d718",
   "metadata": {},
   "source": [
    "然后我们训练这一层线性层的时候就需要计算损失并反向传播，然后更新权值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bee0f317-e5a3-4870-92fb-5bcabd15e86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.5\n",
    "loss = loss_func(preds, y_train)\n",
    "loss.backward()\n",
    "with torch.no_grad():\n",
    "    weights -= weights.grad * lr\n",
    "    bias -= bias.grad * lr\n",
    "    weights.grad.zero_()\n",
    "    bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f7d809-8dd9-4a03-98b7-6aacad77c2d3",
   "metadata": {},
   "source": [
    "如果我们使用nn中准备好的工具，那么整个过程就会变得更加清晰简洁。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4d484582-42d0-40c1-87a8-5e736099d254",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(784, 10)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        return self.lin(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87fed15-0256-4b37-8926-44f86d0cc112",
   "metadata": {},
   "source": [
    "其中，nn.Linear是这样的：\n",
    "\n",
    "![](../img/pytorch_linear.png)\n",
    "\n",
    "- in_features指的是输入的二维张量的大小，即输入的 [batch_size, size] 中的size（输入图片的特征共有多少个，上一个全连接层神经元的个数）。\n",
    "\n",
    "- out_features指的是输出的二维张量的大小，即输出的二维张量的形状为[batch_size，output_size]，当然，它也代表了该全连接层的神经元个数。\n",
    "\n",
    "- Linear其实就是对输入 X<sub>n×i</sub>执行了一个线性变换，即：Y<sub>n×o</sub> = X<sub>n×i</sub>W<sub>i×o</sub> + b ，其中W是模型要学习的参数，W的维度为 W <sub>i×o</sub>，b是o维的向量偏置，n为输入向量的行数（例如，你想一次输入10个样本，即batch_size为10，则n = 10 ），i为输入神经元的个数（例如你的样本特征数为5，则i = 5），o为输出神经元的个数。\n",
    "\n",
    "从输入输出的张量的shape角度来理解，相当于一个输入为[batch_size, in_features]的张量变换成了[batch_size, out_features]的输出张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "69d72e98-57c9-435e-a2e3-c01af7fff05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear()\n",
    "loss = loss_func(model(x_train), y_train)\n",
    "loss.backward()\n",
    "with torch.no_grad():\n",
    "    for p in model.parameters():\n",
    "        p -= p.grad * lr\n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01eafbf-4af1-4112-84b1-7b683b5702a5",
   "metadata": {},
   "source": [
    "事实上，loss_func在nn模块中都有现成的工具，整个过程会更简单，另外，梯度下降的优化算法也有专门的optim包来帮助实现，这些内容在最后一个实例中我们都会看到。总之，用nn等模块会更方便，接下来我们进一步看看最关键的神经网络通过nn包是如何构建的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d6f7fb-5144-4eba-a08c-49acec8f9530",
   "metadata": {},
   "source": [
    "## 2 nn.Module类的使用\n",
    "\n",
    "torch.nn.Module类是所有神经网络模块(modules)的基类，我们用Pytorch写神经网络模型应该继承这个类，并重载__init__和forward函数。\n",
    "\n",
    "1. 把网络中具有可学习参数的层（如全连接层、卷积层等）放在构造函数 \\_\\_init\\_\\_()中；\n",
    "2. 不具有可学习参数的层(如ReLU、dropout、BatchNormanation层)最好也放在构造函数 \\_\\_init\\_\\_()中，如果不放在构造函数__init__里面，则在forward方法里面使用nn.functional来代替\n",
    "3. forward方法是必须要重写的，它来具体实现各个层之间的连接。\n",
    "\n",
    "下面来看一个两个非线性层的简单例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fbd8eff8-4f54-4c9f-a8ca-f735399321d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mlp(\n",
       "  (lin1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (lin2): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class Mlp(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mlp, self).__init__()  # 调用父类的构造函数\n",
    "        self.lin1 = torch.nn.Linear(784, 256)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.lin2 = torch.nn.Linear(256, 10)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Mlp()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4e0ed3-7581-4e92-b85e-129e5c29ee2c",
   "metadata": {},
   "source": [
    "我们稍作解释，因为前面Python基础的地方我们没有涉及类的概念，所以这里我们从类开始说起。\n",
    "\n",
    "类可以简单理解为一系列变量和方法的集合体，比如人物，我们要用代码虚拟化它，需要他/她的各种自然属性和社会属性以及行为，这些都可以用变量和函数来表达，这个集合体就是人这个类，当给这一系列属性赋予确定值时，我们就称之为实例化，这个实例化的产物就称作对象。\n",
    "\n",
    "一个类可以被另一个类继承，例如中国人类继承人类，人这个类的内容中国人类都有，但是还可以包括其他专属于我国的特点。在python中，继承的形式就是上面示例的形式：\n",
    "\n",
    "```Python\n",
    "class Sonclass(ParentClass):\n",
    "    def __init__(self):\n",
    "        ...\n",
    "```\n",
    "\n",
    "那么nn.Module就是Pytorch写好的一个类，我们写的类就继承了它的各类属性、方法，如果父类中一个函数被继承后，子类修改了其内容，那么子类的内容就会覆盖掉父类的，我们正是通过重写构造函数 \\_\\_init\\_\\_()和forward函数来实现神经网络的定制。在前者中，我们把神经网络的各层表达出来，在后者中，我们使用各层去作运算，连接在前者中构建的各层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24511d0c-f572-40b8-8906-36123280db16",
   "metadata": {},
   "source": [
    "上面提到的“不具有可学习参数的层(如ReLU、dropout、BatchNormanation层)如果不放在构造函数__init__里面，则在forward方法里面使用nn.functional来代替”的意思是：如果ReLU等不在构造函数中标出，那么在forward里直接用同等作用的函数直接执行运算即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6ab3d624-78f0-455f-bc19-6cc9c623aa9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mlp(\n",
       "  (lin1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (lin2): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Mlp(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Mlp, self).__init__()  # 调用父类的构造函数\n",
    "        self.lin1 = torch.nn.Linear(784, 256)\n",
    "        self.lin2 = torch.nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = Mlp()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eb6e69-558b-4009-bb2f-b7f522b323a9",
   "metadata": {},
   "source": [
    "两种写法是一样的，关于nn.funtional和nn.Module的区别，大家可以自己去查一查。个人理解，就是没有可学习参数的，可能没区别，有可学习参数的前者不自带神经网络参数，后者则是有参数的。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tutorial)",
   "language": "python",
   "name": "tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
